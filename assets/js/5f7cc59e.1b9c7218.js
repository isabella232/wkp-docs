(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{125:function(e,t,n){"use strict";n.d(t,"a",(function(){return d})),n.d(t,"b",(function(){return h}));var a=n(0),r=n.n(a);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function c(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=r.a.createContext({}),p=function(e){var t=r.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):c(c({},t),e)),n},d=function(e){var t=p(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),d=p(n),b=a,h=d["".concat(i,".").concat(b)]||d[b]||u[b]||o;return n?r.a.createElement(h,c(c({ref:t},l),{},{components:n})):r.a.createElement(h,c({ref:t},l))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=b;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:a,i[1]=c;for(var l=2;l<o;l++)i[l]=n[l];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"},93:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return i})),n.d(t,"toc",(function(){return c})),n.d(t,"default",(function(){return l}));var a=n(3),r=(n(0),n(125));const o={title:"Patching Node OS packages"},i={unversionedId:"tasks/patching-node-os-packages",id:"tasks/patching-node-os-packages",isDocsHomePage:!1,title:"Patching Node OS packages",description:"OS Patching",source:"@site/docs/tasks/patching-node-os-packages.md",slug:"/tasks/patching-node-os-packages",permalink:"/wkp-docs/docs/tasks/patching-node-os-packages",editUrl:"https://github.com/weaveworks/wkp-docs/tree/main/docs/tasks/patching-node-os-packages.md",version:"current",sidebar:"docs",previous:{title:"Specifying Multiple Admins",permalink:"/wkp-docs/docs/tasks/multiple-cluster-admins"},next:{title:"Profiling kubectl",permalink:"/wkp-docs/docs/tasks/profiling-kubectl"}},c=[{value:"OS Patching",id:"os-patching",children:[]},{value:"Cluster/Node lifecycle",id:"clusternode-lifecycle",children:[{value:"Patching in-place",id:"patching-in-place",children:[]},{value:"Replace/Cycle nodes",id:"replacecycle-nodes",children:[]}]},{value:"Add/Remove/Drain nodes",id:"addremovedrain-nodes",children:[]}],s={toc:c};function l({components:e,...t}){return Object(r.b)("wrapper",Object(a.a)({},s,t,{components:e,mdxType:"MDXLayout"}),Object(r.b)("h3",{id:"os-patching"},"OS Patching"),Object(r.b)("p",null,"The machines backing WKP Nodes can be managed at the OS level in several ways.\nWhether you are provisioning the base OS with cloud-init, kickstart, a shell script,\nbaking OS images, or using a configuration-management system like Ansible/Chef/Puppet/Salt,\nthere are some things you will want to consider when patching OS packages in your WKP\ncluster."),Object(r.b)("p",null,"Kubernetes related dependencies such as Docker, Containerd, and the Kubelet are\ndeclaratively managed by the WKS-Controller via the ",Object(r.b)("inlineCode",{parentName:"p"},"Cluster")," ","[",Object(r.b)("inlineCode",{parentName:"p"},"ExistingInfraCluster"),"]",'({{< ref "/reference/cluster" >}}).\nThese packages are normally updated with a cluster upgrade.\nIf you find you\'d like to patch WKP managed packages out-of-band for a ',Object(r.b)("strong",{parentName:"p"},"bug-fix")," or ",Object(r.b)("strong",{parentName:"p"},"CVE"),",\n",Object(r.b)("em",{parentName:"p"},"please contact WKP Support")," to avoid service disruption."),Object(r.b)("p",null,"Packages such as the OS kernel and SSHD (which is a pre-requisite for the SSH Cluster API Provider)\nare installed beforehand and are not managed by WKP.\nPlease feel free to update these packages in whichever way you see fit.\nYour upstream linux support should provide patches for CVE's in a timely manner."),Object(r.b)("p",null,"Using a git-backed, pull-based configuration management system like Chef, Puppet, or Salt is\nin line with GitOps principles if you have a mutable OS and are patching in-place."),Object(r.b)("h2",{id:"clusternode-lifecycle"},"Cluster/Node lifecycle"),Object(r.b)("p",null,"There are two primary package update strategies. "),Object(r.b)("h3",{id:"patching-in-place"},"Patching in-place"),Object(r.b)("p",null,"You can choose to upgrade your packages by patching them using a package manager like\napt or yum. The WKS-Controller uses this mechanism for cluster upgrades if you are not\ncycling out your Nodes via git.\nUpdating packages in-place can be a great, practical option for large clusters when the\nprovision time for new machines is slow and cluster capacity is inflexible or oversubscribed."),Object(r.b)("p",null,"If a package requires a restart of linux, you may consider tainting or cordoning Nodes\nto prevent kube-scheduler from assigning new Pods to them and draining critical workloads\nbefore issuing a Node restart.\nYou can run ",Object(r.b)("a",{parentName:"p",href:"https://github.com/weaveworks/kured"},"weaveworks/kured")," in the cluster to\nautomate the detection, taint, drain, and reboot process across all Nodes with a\ndeclarative lifecycle."),Object(r.b)("p",null,"If you are looking to patch more quickly, causing a disruption to the Pods on those Nodes\nwith an OS restart may be within your SLAs; for instance, if you have the proper\n",Object(r.b)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/run-application/configure-pdb/"},Object(r.b)("inlineCode",{parentName:"a"},"PodDisruptionBudgets"))," in place beforehand\nor have redundant routing/load-balancing with circuit breakers, along with zero-downtime\nhandling of Pod Lifecycle",Object(r.b)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/"},"[1]"),"/",Object(r.b)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=0o5C12kzEDI"},"[2]"),",\nyou may observe little to no service disruption."),Object(r.b)("h3",{id:"replacecycle-nodes"},"Replace/Cycle nodes"),Object(r.b)("p",null,"If you have a fast machine provisioner, are baking golden images, or are leveraging\nimmutable infrastructure, replacing and cycling out your Nodes with patched ones may be\nmore appropriate. "),Object(r.b)("p",null,"This procedure is similar to the ",Object(r.b)("inlineCode",{parentName:"p"},"RollingUpdate")," strategy used by the ",Object(r.b)("inlineCode",{parentName:"p"},"Deployment"),"\ncontroller for ",Object(r.b)("inlineCode",{parentName:"p"},"ReplicaSets")," and their ",Object(r.b)("inlineCode",{parentName:"p"},"Pods"),", but carried out instead using sets of Nodes."),Object(r.b)("p",null,"The strategy is to add new Nodes to the cluster using the new base image.\nYou then migrate workloads to them by draining old Nodes.\nOnce the workloads are migrated, you can remove the old Nodes, and repeat the process in phases."),Object(r.b)("p",null,"Having more free or elastic capacity to provision new machines will make rollout of new\nNodes quicker and easier.\nIf you are able to double the cluster capacity temporarily, you can complete this rollout\nin 1 phase."),Object(r.b)("h2",{id:"addremovedrain-nodes"},"Add/Remove/Drain nodes"),Object(r.b)("p",null,"You can ",Object(r.b)("strong",{parentName:"p"},"add")," or ",Object(r.b)("strong",{parentName:"p"},"remove")," cluster Nodes by committing to the ","[Machines manifest]",'({{< ref "/reference/machines" >}}).',Object(r.b)("br",{parentName:"p"}),"\n","Nodes can be ",Object(r.b)("strong",{parentName:"p"},"tainted")," by committing to that same Machines manifest.\n",Object(r.b)("strong",{parentName:"p"},"taints")," can be used to repel new workloads from the Node."),Object(r.b)("p",null,"Take care to not unintentionally taint, add or remove Nodes in the same commit.\nNew Nodes will need time to provision and become ",Object(r.b)("inlineCode",{parentName:"p"},"Ready"),". They may not provision\nsuccessfully, which can leave new Pods in an unschedulable state. "),Object(r.b)("p",null,"You can drain Nodes using ",Object(r.b)("inlineCode",{parentName:"p"},"kubectl drain"),'.\nThe drain command will "cordon" the Node which marks it as UnSchedulable.\nThis acts similarly to taints, but Pods cannot ignore it.\nSee the ',Object(r.b)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/"},"Kubernetes docs"),"\nfor notes on Pod Eviction behavior and ",Object(r.b)("inlineCode",{parentName:"p"},"PodDisruptionBudgets"),"."))}l.isMDXComponent=!0}}]);